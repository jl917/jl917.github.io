# LLM Judge

用一个大模型来当裁判，评判其他大模型的输出质量。 你给他一个评估标准，然后让它根据这个标准来评分。是否准确，是否完整，是否符合要求等等。

### 优点

- 规模化
  - 人工评判成本高且耗时，使用大模型作为裁判可以大幅降低成本并提高效率。
- 一致性
  - 人类评判可能会受到主观因素影响，而大模型可以提供更加一致和客观的评判。
- 成本低
  - 相比于雇佣大量人类评判员，使用大模型作为裁判的成本更低。
- 可解释性
  - 许多大模型可以提供评判依据和解释，帮助理解评分结果。

### 偏见

- 位置偏见
  - 靠前的答案可能会被认为更好。
  - 解决: 在评分时随机打乱答案顺序， 评估多次取平均， 次结果差异大时人工复核。
- 长度偏见
  - 更长的答案可能会被认为更全面。
  - 解决: 评估prompt中明确告诉他评分标准

  ```markdown
  # 评分标准说明

  回答的质量与长度无关

  评分原则
  简洁准确的回答 -> 高分
  冗长啰嗦的回答 -> 低分
  ```

- 自我偏好
  - 模型可能会偏好与自己生成风格相似的答案。
  - 解决: 用不同的模型交叉评估
- 风格偏见
  - 模型可能会偏好某种特定的表达风格。比如说 结构化分点回答比连贯自然语言分数更高。
  - 解决: 在评分标准中明确说明检查的内容
    - 明确评估标准
      - 关注内容准确性
      - 完整性是核心评分纬度
      - 表达形式不影响评分
    - 提供标准示例
      - 展示不同风格的高质量回答
      - 帮助模型理解质量 不同于 形式
- 知识边界
  - 模型可能无法准确评估超出其知识范围的答案。 但模型会很自信的给出评估结果，这会导致误导。
  - 解决: 引入领域知识，prompt里给相关的背景资料， 或者这个领域的微调好的模型来评估。 要是跳前不允许 只能人工评估。
